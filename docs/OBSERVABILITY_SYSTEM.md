# üîç Sistema de Observabilidad Avanzada

## üìã Tabla de Contenidos

- [üéØ Vista General](#-vista-general)
- [üèóÔ∏è Arquitectura del Sistema](#Ô∏è-arquitectura-del-sistema)
- [üìä Diagrama de Flujo](#-diagrama-de-flujo)
- [üîß API Reference](#-api-reference)
- [üîÑ Flujos Detallados](#-flujos-detallados)
- [üí° Casos de Uso](#-casos-de-uso)
- [üöÄ Gu√≠a de Implementaci√≥n](#-gu√≠a-de-implementaci√≥n)
- [üß™ Testing y Validaci√≥n](#-testing-y-validaci√≥n)

---

## üéØ Vista General

El **Sistema de Observabilidad Avanzada** es una implementaci√≥n completa de m√©tricas, trazabilidad y monitoreo para agentes ReAct basados en LangGraph. Proporciona visibilidad completa del rendimiento, costo y salud del sistema en tiempo real.

### ‚ú® Caracter√≠sticas Principales

- **üìä M√©tricas Avanzadas**: P50/P95/P99 latencia, tokens, costo estimado
- **üîó Trazabilidad**: Correlation IDs √∫nicos, spans anidados
- **üéØ KPIs en Tiempo Real**: Performance, reliability, efficiency, health
- **üö® SLOs y Alertas**: Service Level Objectives autom√°ticos
- **üõ†Ô∏è Herramienta de Monitoreo**: Interfaz program√°tica para m√©tricas
- **üìà An√°lisis de Tendencias**: Hist√≥rico y predicciones b√°sicas

---

## üèóÔ∏è Arquitectura del Sistema

```mermaid
graph TB
    subgraph "Agente ReAct"
        A[Planner] --> B[Reasoner]
        B --> C[Tool Selector]
        C --> D[Tool Executor]
        D --> E[Critic]
        E --> F[Finalizer]
    end
    
    subgraph "Sistema de Observabilidad"
        G[MetricsCollector] --> H[Metrics]
        I[TraceContext] --> J[Span Manager]
        K[MetricsDashboard] --> L[SLOs & Alerts]
        M[MonitorTool] --> N[External APIs]
    end
    
    subgraph "Integraci√≥n"
        A -.->|span()| G
        B -.->|span()| G
        C -.->|span()| G
        D -.->|span()| G
        E -.->|span()| G
        F -.->|span()| G
        
        G --> K
        I --> J
        J --> G
        K --> M
    end
    
    style G fill:#e1f5fe
    style K fill:#f3e5f5
    style M fill:#e8f5e8
```

### üîß Componentes Principales

1. **MetricsCollector**: Recolector centralizado de m√©tricas
2. **TraceContext**: Gesti√≥n de trazabilidad y correlation IDs
3. **MetricsDashboard**: Dashboard de KPIs y SLOs
4. **MonitorTool**: Herramienta de monitoreo program√°tico
5. **Span Manager**: Gesti√≥n de spans y m√©tricas de operaciones

---

## üìä Diagrama de Flujo

```mermaid
flowchart TD
    A[Usuario ejecuta agente] --> B[span() inicia]
    B --> C[TraceContext.new_span()]
    C --> D[Record start time]
    D --> E[Execute operation]
    E --> F{Success?}
    
    F -->|Yes| G[Record success metrics]
    F -->|No| H[Record error metrics]
    
    G --> I[Calculate percentiles]
    H --> I
    I --> J[Update dashboard]
    J --> K[Check SLOs]
    K --> L{Any violations?}
    
    L -->|Yes| M[Create alerts]
    L -->|No| N[Update KPIs]
    
    M --> N
    N --> O[Export metrics]
    O --> P[End span]
```

---

## üîß API Reference

### üìä Clase `Metrics`

**Prop√≥sito**: Almacena m√©tricas de rendimiento y costo para una operaci√≥n.

**Atributos**:
```python
class Metrics:
    # Latencia (en milisegundos)
    latency_p50: float = 0.0      # Percentil 50
    latency_p95: float = 0.0      # Percentil 95
    latency_p99: float = 0.0      # Percentil 99
    
    # Tokens y costo
    input_tokens: int = 0          # Tokens de entrada
    output_tokens: int = 0         # Tokens de salida
    total_tokens: int = 0          # Total de tokens
    estimated_cost_usd: float = 0.0 # Costo estimado en USD
    
    # Rendimiento
    total_steps: int = 0           # Total de pasos
    successful_steps: int = 0      # Pasos exitosos
    failed_steps: int = 0          # Pasos fallidos
    tool_calls: int = 0            # Llamadas a herramientas
    tool_success_rate: float = 0.0 # Tasa de √©xito de herramientas
    
    # Errores
    error_count: int = 0           # Conteo de errores
    error_rate: float = 0.0        # Tasa de error
    
    # Recursos
    memory_usage_mb: float = 0.0   # Uso de memoria en MB
    cpu_usage_percent: float = 0.0 # Uso de CPU en porcentaje
```

**Ejemplo de uso**:
```python
from src.core.observability import Metrics

# Crear m√©tricas personalizadas
metrics = Metrics(
    latency_p50=100.0,
    total_tokens=500,
    total_steps=10
)

print(f"Latencia P50: {metrics.latency_p50}ms")
print(f"Total tokens: {metrics.total_tokens}")
```

### üîó Clase `TraceContext`

**Prop√≥sito**: Gestiona el contexto de trazabilidad para correlacionar logs y m√©tricas.

**Atributos**:
```python
class TraceContext:
    trace_id: str                    # ID √∫nico de la traza
    span_id: str                     # ID del span actual
    parent_span_id: str | None       # ID del span padre
    user_id: str | None              # ID del usuario
    session_id: str | None           # ID de la sesi√≥n
    request_id: str | None           # ID de la request
```

**M√©todos**:
```python
def new_span(self) -> "TraceContext":
    """Crea un nuevo span hijo."""
    return TraceContext(
        trace_id=self.trace_id,
        parent_span_id=self.span_id,
        user_id=self.user_id,
        session_id=self.session_id,
        request_id=self.request_id
    )
```

**Ejemplo de uso**:
```python
from src.core.observability import TraceContext

# Crear contexto ra√≠z
root_context = TraceContext()
print(f"Trace ID: {root_context.trace_id}")

# Crear span hijo
child_context = root_context.new_span()
print(f"Parent Span ID: {child_context.parent_span_id}")
```

### üìà Clase `MetricsCollector`

**Prop√≥sito**: Recolector centralizado de m√©tricas del sistema.

**M√©todos principales**:
```python
class MetricsCollector:
    def record_latency(self, operation: str, latency_ms: float):
        """Registra latencia de una operaci√≥n."""
        
    def record_tokens(self, operation: str, input_tokens: int, output_tokens: int):
        """Registra uso de tokens."""
        
    def record_step(self, operation: str, success: bool, tool_call: bool = False):
        """Registra un paso de ejecuci√≥n."""
        
    def get_summary(self, operation: str = None) -> dict[str, Any]:
        """Obtiene resumen de m√©tricas."""
```

**Ejemplo de uso**:
```python
from src.core.observability import MetricsCollector

collector = MetricsCollector()

# Registrar m√©tricas
collector.record_latency("planner", 150.5)
collector.record_tokens("planner", 100, 50)
collector.record_step("planner", success=True, tool_call=False)

# Obtener resumen
summary = collector.get_summary()
print(f"Operaciones: {len(summary['operations'])}")
```

### üéØ Clase `SLODefinition`

**Prop√≥sito**: Define Service Level Objectives para monitoreo autom√°tico.

**Atributos**:
```python
class SLODefinition:
    name: str           # Nombre del SLO
    metric: str         # M√©trica a monitorear
    threshold: float    # Umbral objetivo
    operator: str       # Operador (>=, <=, ==, !=)
    severity: str       # Severidad (info, warning, error, critical)
    description: str    # Descripci√≥n del SLO
```

**M√©todos**:
```python
def evaluate(self, value: float) -> bool:
    """Eval√∫a si el valor cumple con el SLO."""
    if self.operator == ">=":
        return value >= self.threshold
    elif self.operator == "<=":
        return value <= self.threshold
    elif self.operator == "==":
        return value == self.threshold
    elif self.operator == "!=":
        return value != self.threshold
    return False
```

**Ejemplo de uso**:
```python
from src.core.metrics_dashboard import SLODefinition

# SLO de latencia
latency_slo = SLODefinition(
    "latency_p95",
    "latency_p95",
    5000.0,  # 5 segundos
    "<=",
    "warning",
    "P95 de latencia debe estar bajo 5 segundos"
)

# Evaluar SLO
current_latency = 3000.0
is_compliant = latency_slo.evaluate(current_latency)
print(f"SLO cumplido: {is_compliant}")
```

### üìä Clase `MetricsDashboard`

**Prop√≥sito**: Dashboard principal para visualizaci√≥n y alertas de m√©tricas.

**M√©todos principales**:
```python
class MetricsDashboard:
    def add_slo(self, name: str, metric: str, threshold: float, 
                 operator: str = ">=", severity: str = "warning", 
                 description: str = ""):
        """Agrega un nuevo SLO."""
        
    def evaluate_slos(self, metrics: dict[str, Any]) -> list[dict[str, Any]]:
        """Eval√∫a todos los SLOs contra las m√©tricas actuales."""
        
    def update(self) -> dict[str, Any]:
        """Actualiza el dashboard con m√©tricas frescas."""
        
    def get_kpis(self) -> dict[str, Any]:
        """Obtiene KPIs clave para monitoreo."""
        
    def acknowledge_alert(self, alert_id: str):
        """Marca una alerta como reconocida."""
        
    def export_metrics(self, format: str = "json") -> str:
        """Exporta m√©tricas en diferentes formatos."""
        
    def get_metrics_trend(self, hours: int = 24) -> dict[str, Any]:
        """Obtiene tendencias de m√©tricas en las √∫ltimas N horas."""
```

**Ejemplo de uso**:
```python
from src.core.metrics_dashboard import MetricsDashboard

dashboard = MetricsDashboard()

# Agregar SLO personalizado
dashboard.add_slo(
    "custom_latency",
    "latency_p95",
    3000.0,
    "<=",
    "error",
    "Latencia P95 debe estar bajo 3 segundos"
)

# Actualizar dashboard
update_result = dashboard.update()
print(f"Alertas activas: {update_result['dashboard']['active_alerts']}")

# Obtener KPIs
kpis = dashboard.get_kpis()
print(f"Performance: {kpis['performance']}")
```

### üõ†Ô∏è Clase `MonitorTool`

**Prop√≥sito**: Herramienta de monitoreo que permite al agente consultar m√©tricas program√°ticamente.

**Atributos**:
```python
class MonitorTool:
    name: str = "monitor"
    description: str = "Monitorea m√©tricas, KPIs y estado del sistema de observabilidad."
    timeout_s: int = 5
```

**M√©todos**:
```python
def __call__(self, args: MonitorIn) -> ToolOutput:
    """Ejecuta la acci√≥n de monitoreo solicitada."""
    
def _get_kpis(self) -> dict:
    """Obtiene KPIs clave del sistema."""
    
def _get_metrics(self, operation: str = None) -> dict:
    """Obtiene m√©tricas detalladas."""
    
def _get_dashboard(self) -> dict:
    """Obtiene estado completo del dashboard."""
    
def _get_alerts(self) -> dict:
    """Obtiene alertas activas del sistema."""
    
def _get_trends(self, hours: int) -> dict:
    """Obtiene tendencias de m√©tricas."""
```

**Ejemplo de uso**:
```python
from src.tools.monitor import MonitorTool, MonitorIn

monitor = MonitorTool()

# Obtener KPIs
result = monitor(MonitorIn(action="kpis", format="summary"))
print(f"KPIs: {result.content}")

# Obtener m√©tricas de una operaci√≥n espec√≠fica
result = monitor(MonitorIn(action="metrics", operation="planner"))
print(f"M√©tricas del planner: {result.content}")

# Obtener alertas
result = monitor(MonitorIn(action="alerts"))
print(f"Alertas: {result.content}")
```

---

## üîÑ Flujos Detallados

### üìä Flujo de M√©tricas B√°sicas

```mermaid
sequenceDiagram
    participant U as Usuario
    participant A as Agente
    participant S as Span
    participant M as MetricsCollector
    participant D as Dashboard
    
    U->>A: Ejecuta operaci√≥n
    A->>S: span("operation_name")
    S->>M: record_latency()
    S->>M: record_tokens()
    S->>M: record_step()
    S->>D: update()
    D->>D: evaluate_slos()
    D->>D: create_alerts()
    S->>A: Retorna resultado
    A->>U: Respuesta final
```

**Ejemplo de c√≥digo**:
```python
from src.core.observability import span, get_metrics_summary

def mi_operacion():
    with span("mi_operacion") as span_ctx:
        # Tu l√≥gica aqu√≠
        resultado = hacer_algo()
        
        # Las m√©tricas se registran autom√°ticamente
        return resultado

# Ejecutar y ver m√©tricas
mi_operacion()
metrics = get_metrics_summary()
print(f"Operaciones: {metrics['operations']}")
```

### üéØ Flujo de SLOs y Alertas

```mermaid
sequenceDiagram
    participant M as Metrics
    participant D as Dashboard
    participant S as SLOs
    participant A as Alerts
    
    M->>D: update()
    D->>S: evaluate_slos()
    S->>S: check_thresholds()
    S->>A: create_violations()
    A->>A: set_severity()
    A->>D: add_alert()
    D->>D: update_kpis()
```

**Ejemplo de c√≥digo**:
```python
from src.core.metrics_dashboard import update_dashboard, get_kpis

# Actualizar dashboard (eval√∫a SLOs autom√°ticamente)
dashboard_update = update_dashboard()

# Verificar violaciones
violations = dashboard_update['violations']
if violations:
    print(f"‚ö†Ô∏è {len(violations)} SLOs violados:")
    for violation in violations:
        print(f"  - {violation['slo_name']}: {violation['message']}")

# Obtener KPIs actualizados
kpis = get_kpis()
print(f"Health score: {kpis['health']}")
```

### üîç Flujo de Monitoreo Program√°tico

```mermaid
sequenceDiagram
    participant A as Agente
    participant M as MonitorTool
    participant D as Dashboard
    participant O as Observability
    
    A->>M: MonitorIn(action="kpis")
    M->>D: get_kpis()
    D->>O: get_metrics_summary()
    O->>D: return metrics
    D->>M: format_kpis()
    M->>A: ToolOutput(content)
```

**Ejemplo de c√≥digo**:
```python
from src.tools.monitor import MonitorTool, MonitorIn

# El agente puede monitorear su propio estado
monitor = MonitorTool()

# Verificar KPIs
kpis_result = monitor(MonitorIn(action="kpis"))
if kpis_result.ok:
    kpis = kpis_result.content
    if kpis['health']['active_alerts'] > 0:
        print("‚ö†Ô∏è Hay alertas activas en el sistema")
    
    if kpis['performance']['latency_p95_ms'] > 5000:
        print("üêå La latencia P95 est√° alta")
```

---

## üí° Casos de Uso

### üöÄ Caso 1: Monitoreo de Agente ReAct

**Escenario**: Quieres monitorear el rendimiento de tu agente ReAct en tiempo real.

**Implementaci√≥n**:
```python
from src.core.observability import span, get_metrics_summary
from src.core.metrics_dashboard import get_kpis

def agente_react():
    with span("react_episode") as episode_span:
        # Planificaci√≥n
        with span("planner"):
            plan = crear_plan()
        
        # Razonamiento
        with span("reasoner"):
            razonamiento = analizar_situacion()
        
        # Ejecuci√≥n de herramientas
        with span("tool_executor"):
            resultado = ejecutar_herramienta()
        
        # Cr√≠tica
        with span("critic"):
            evaluacion = evaluar_resultado()
        
        return resultado

# Ejecutar agente
resultado = agente_react()

# Ver m√©tricas inmediatamente
metrics = get_metrics_summary()
kpis = get_kpis()

print("üìä Rendimiento del Agente:")
print(f"  Latencia P95: {kpis['performance']['latency_p95_ms']:.1f}ms")
print(f"  Tasa de √©xito: {kpis['reliability']['success_rate']*100:.1f}%")
print(f"  Costo estimado: ${kpis['efficiency']['estimated_cost_usd']:.4f}")
```

### üéØ Caso 2: Alertas Autom√°ticas de SLOs

**Escenario**: Quieres recibir alertas cuando el sistema no cumple con los objetivos de calidad.

**Implementaci√≥n**:
```python
from src.core.metrics_dashboard import MetricsDashboard, update_dashboard

# Configurar SLOs personalizados
dashboard = MetricsDashboard()

# SLO de latencia cr√≠tica
dashboard.add_slo(
    "latencia_critica",
    "latency_p95",
    2000.0,  # 2 segundos
    "<=",
    "critical",
    "Latencia P95 cr√≠tica - debe estar bajo 2 segundos"
)

# SLO de tasa de √©xito
dashboard.add_slo(
    "tasa_exito_minima",
    "tool_success_rate",
    0.95,  # 95%
    ">=",
    "error",
    "Tasa de √©xito m√≠nima del 95%"
)

# Monitorear continuamente
def monitorear_slos():
    while True:
        update_result = update_dashboard()
        
        # Verificar alertas cr√≠ticas
        for alert in update_result['recent_alerts']:
            if alert['severity'] == 'critical':
                print(f"üö® ALERTA CR√çTICA: {alert['message']}")
                # Aqu√≠ podr√≠as enviar notificaci√≥n por email/Slack
        
        time.sleep(60)  # Verificar cada minuto

# Iniciar monitoreo en background
import threading
monitor_thread = threading.Thread(target=monitorear_slos, daemon=True)
monitor_thread.start()
```

### üìà Caso 3: An√°lisis de Tendencias

**Escenario**: Quieres analizar c√≥mo evoluciona el rendimiento del sistema a lo largo del tiempo.

**Implementaci√≥n**:
```python
from src.core.metrics_dashboard import get_dashboard
from datetime import datetime, timedelta

def analizar_tendencias():
    dashboard = get_dashboard()
    
    # Tendencias de las √∫ltimas 24 horas
    trends_24h = dashboard.get_metrics_trend(24)
    
    print("üìà Tendencias (24h):")
    for metric, data in trends_24h['trends'].items():
        trend = data['trend']
        avg = data['avg']
        
        if trend == 'increasing':
            emoji = "üìà"
        elif trend == 'decreasing':
            emoji = "üìâ"
        else:
            emoji = "‚û°Ô∏è"
        
        print(f"  {emoji} {metric}: {avg:.2f} ({trend})")
    
    # Tendencias de la √∫ltima semana
    trends_week = dashboard.get_metrics_trend(168)  # 7 * 24 horas
    
    print("\nüìä Tendencias (1 semana):")
    for metric, data in trends_week['trends'].items():
        print(f"  {metric}: {data['trend']} (promedio: {data['avg']:.2f})")

# Ejecutar an√°lisis
analizar_tendencias()
```

### üõ†Ô∏è Caso 4: Integraci√≥n con Herramientas Externas

**Escenario**: Quieres exportar m√©tricas a sistemas de monitoreo externos como Grafana o Prometheus.

**Implementaci√≥n**:
```python
from src.core.metrics_dashboard import get_dashboard
import json
import requests

def exportar_a_prometheus():
    """Exporta m√©tricas en formato Prometheus."""
    dashboard = get_dashboard()
    metrics = dashboard.get_metrics_summary()
    
    prometheus_metrics = []
    
    # M√©tricas globales
    global_metrics = metrics['global_metrics']
    prometheus_metrics.append(f"# HELP total_steps Total steps executed")
    prometheus_metrics.append(f"# TYPE total_steps counter")
    prometheus_metrics.append(f"total_steps {global_metrics['total_steps']}")
    
    prometheus_metrics.append(f"# HELP total_tokens Total tokens used")
    prometheus_metrics.append(f"# TYPE total_tokens counter")
    prometheus_metrics.append(f"total_tokens {global_metrics['total_tokens']}")
    
    prometheus_metrics.append(f"# HELP estimated_cost_usd Estimated cost in USD")
    prometheus_metrics.append(f"# TYPE estimated_cost_usd gauge")
    prometheus_metrics.append(f"estimated_cost_usd {global_metrics['estimated_cost_usd']}")
    
    # M√©tricas por operaci√≥n
    for op_name, op_metrics in metrics['operations'].items():
        prometheus_metrics.append(f"# HELP operation_latency_p95 Latency P95 by operation")
        prometheus_metrics.append(f"# TYPE operation_latency_p95 gauge")
        prometheus_metrics.append(f'operation_latency_p95{{operation="{op_name}"}} {op_metrics["latency_p95"]}')
    
    return "\n".join(prometheus_metrics)

def exportar_a_grafana():
    """Exporta m√©tricas a Grafana via API."""
    dashboard = get_dashboard()
    kpis = dashboard.get_kpis()
    
    # Preparar datos para Grafana
    grafana_data = {
        "time": datetime.now().isoformat(),
        "performance": kpis['performance'],
        "reliability": kpis['reliability'],
        "efficiency": kpis['efficiency'],
        "health": kpis['health']
    }
    
    # Enviar a Grafana (ejemplo)
    # response = requests.post(
    #     "http://grafana:3000/api/datasources/proxy/1/write",
    #     json=grafana_data
    # )
    
    return grafana_data

# Exportar m√©tricas
prometheus_metrics = exportar_a_prometheus()
print("üìä M√©tricas Prometheus:")
print(prometheus_metrics)

grafana_data = exportar_a_grafana()
print("\nüìà Datos para Grafana:")
print(json.dumps(grafana_data, indent=2))
```

---

## üöÄ Gu√≠a de Implementaci√≥n

### üì¶ Instalaci√≥n

```bash
# Instalar dependencias
pip install -e ".[dev]"

# Verificar instalaci√≥n
python -c "from src.core.observability import span; print('‚úÖ Observabilidad instalada')"
```

### üîß Configuraci√≥n B√°sica

```python
# 1. Importar componentes principales
from src.core.observability import span, get_metrics_summary, reset_metrics
from src.core.metrics_dashboard import get_kpis, update_dashboard
from src.tools.monitor import MonitorTool, MonitorIn

# 2. Resetear m√©tricas para empezar limpio
reset_metrics()

# 3. Usar span() para monitorear operaciones
with span("mi_operacion"):
    # Tu c√≥digo aqu√≠
    resultado = hacer_algo()
    
# 4. Ver m√©tricas inmediatamente
metrics = get_metrics_summary()
print(f"Operaciones monitoreadas: {len(metrics['operations'])}")
```

### üéØ Configuraci√≥n Avanzada

```python
from src.core.metrics_dashboard import MetricsDashboard

# Crear dashboard personalizado
dashboard = MetricsDashboard()

# Agregar SLOs personalizados
dashboard.add_slo(
    "mi_slo_critico",
    "latency_p95",
    1000.0,  # 1 segundo
    "<=",
    "critical",
    "Latencia cr√≠tica para mi aplicaci√≥n"
)

# Configurar alertas personalizadas
def mi_callback_alertas(violations):
    for violation in violations:
        if violation['severity'] == 'critical':
            print(f"üö® CR√çTICO: {violation['message']}")
            # Enviar notificaci√≥n inmediata

# Integrar con el dashboard
dashboard.violation_callback = mi_callback_alertas
```

### üîÑ Integraci√≥n con Agentes ReAct

```python
from src.core.observability import span, track_metrics
from src.react.loop import react_loop

# Opci√≥n 1: Usar span() manualmente
def mi_agente_react():
    with span("react_episode"):
        # Tu l√≥gica ReAct aqu√≠
        pass

# Opci√≥n 2: Usar decorador @track_metrics
@track_metrics("mi_agente")
def mi_agente_react():
    # Tu l√≥gica ReAct aqu√≠
    pass

# Opci√≥n 3: Integrar con LangGraph
from src.adapters.langgraph_builder import compile_graph

# El sistema de observabilidad se integra autom√°ticamente
graph = compile_graph(mode="static")
```

---

## üß™ Testing y Validaci√≥n

### ‚úÖ Tests Unitarios

```python
import pytest
from src.core.observability import Metrics, TraceContext, MetricsCollector

def test_metrics_initialization():
    """Test de inicializaci√≥n de m√©tricas."""
    metrics = Metrics()
    assert metrics.latency_p50 == 0.0
    assert metrics.total_steps == 0

def test_trace_context():
    """Test de contexto de trazabilidad."""
    context = TraceContext()
    assert context.trace_id is not None
    assert context.span_id is not None

def test_metrics_collector():
    """Test del recolector de m√©tricas."""
    collector = MetricsCollector()
    collector.record_latency("test", 100.0)
    collector.record_step("test", success=True)
    
    summary = collector.get_summary()
    assert "test" in summary["operations"]
```

### üîç Tests de Integraci√≥n

```python
def test_full_observability_flow():
    """Test del flujo completo de observabilidad."""
    
    # 1. Ejecutar operaciones con spans
    with span("test_operation_1"):
        pass
    
    with span("test_operation_2"):
        pass
    
    # 2. Obtener m√©tricas
    summary = get_metrics_summary()
    assert len(summary["operations"]) == 2
    
    # 3. Actualizar dashboard
    dashboard_update = update_dashboard()
    assert "dashboard" in dashboard_update
    
    # 4. Obtener KPIs
    kpis = get_kpis()
    assert "performance" in kpis
    assert "reliability" in kpis
```

### üöÄ Tests de Rendimiento

```python
import time
from src.core.observability import span

def test_performance_overhead():
    """Test del overhead de rendimiento del sistema de observabilidad."""
    
    # Sin observabilidad
    start = time.time()
    for _ in range(1000):
        pass
    time_without = time.time() - start
    
    # Con observabilidad
    start = time.time()
    for _ in range(1000):
        with span("test"):
            pass
    time_with = time.time() - start
    
    # El overhead debe ser m√≠nimo (< 1ms por operaci√≥n)
    overhead_per_op = (time_with - time_without) / 1000
    assert overhead_per_op < 0.001  # < 1ms
```

---

## üìö Recursos Adicionales

### üîó Enlaces √ötiles

- [Documentaci√≥n de Pre-commit](https://pre-commit.com/)
- [Documentaci√≥n de Ruff](https://docs.astral.sh/ruff/)
- [Google Docstring Style Guide](https://google.github.io/styleguide/pyguide.html#38-comments-and-docstrings)
- [Mermaid Diagram Syntax](https://mermaid.js.org/syntax/flowchart.html)

### üìñ Lecturas Recomendadas

1. **"Observability Engineering"** - Charity Majors
2. **"Site Reliability Engineering"** - Google
3. **"Monitoring Distributed Systems"** - Brendan Gregg

### üõ†Ô∏è Herramientas Relacionadas

- **Grafana**: Visualizaci√≥n de m√©tricas
- **Prometheus**: Almacenamiento de m√©tricas
- **Jaeger**: Trazabilidad distribuida
- **ELK Stack**: Logging y an√°lisis

---

## ü§ù Contribuci√≥n

Para contribuir al sistema de observabilidad:

1. **Fork** el repositorio
2. **Crea** una rama para tu feature
3. **Implementa** tus cambios
4. **A√±ade** tests
5. **Ejecuta** pre-commit hooks
6. **Env√≠a** un pull request

### üìã Checklist de Contribuci√≥n

- [ ] C√≥digo sigue las convenciones del proyecto
- [ ] Tests pasan (`python -m pytest`)
- [ ] Pre-commit hooks pasan (`pre-commit run --all-files`)
- [ ] Documentaci√≥n actualizada
- [ ] Changelog actualizado

---

## üìÑ Licencia

Este proyecto est√° bajo la licencia MIT. Ver [LICENSE](../LICENSE) para m√°s detalles.

---

*√öltima actualizaci√≥n: Agosto 2024*
*Versi√≥n del documento: 1.0.0*
